{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang = 'en'\n",
    "target = 'sentiment'\n",
    "ds = pd.read_csv('../dataset/wiki/opinions_annotated.csv')\n",
    "ds = ds[ds.lang==lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build train and test dataset for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "-1.0    259\n",
       " 0.0    608\n",
       " 1.0    134\n",
       "dtype: int64"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stance_dist=ds.groupby(target).size()\n",
    "stance_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the labels are balanced we take all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1001, 31)"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = ds[~ds[target].isnull()]\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = sample[['text', 'type']]\n",
    "y = sample[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split in train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: (600, 2) , test size: (401, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f'train size: {X_train.shape} , test size: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents='ascii', sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "text features: 1267\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', strip_accents='ascii')\n",
    "X_train_vec = vectorizer.fit_transform(X_train.text)\n",
    "X_test_vec = vectorizer.transform(X_test.text)\n",
    "print(vectorizer)\n",
    "print(f'text features: {X_train_vec.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build and evaluate classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_scores = {}\n",
    "X_test = X_test.copy()\n",
    "X_test['y_test'] = y_test\n",
    "\n",
    "def benchmark_model(name):\n",
    "    print(clf.fit(X_train_vec,y_train))\n",
    "    y_pred = clf.predict(X_test_vec)\n",
    "    X_test.loc[:,name] = y_pred\n",
    "    score = metrics.accuracy_score(y_test, y_pred)\n",
    "    f1_micro = metrics.f1_score(y_test, y_pred, average='micro')\n",
    "    f1_macro = metrics.f1_score(y_test, y_pred, average='macro')\n",
    "    acc_scores[name]=(name, score, f1_micro, f1_macro)\n",
    "    print(f'score: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyClassifier(constant=None, random_state=None, strategy='stratified')\n",
      "score: 0.47381546134663344\n"
     ]
    }
   ],
   "source": [
    "clf = DummyClassifier(strategy='stratified')\n",
    "benchmark_model('random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "score: 0.5910224438902744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "benchmark_model('NB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build SGD classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "score: 0.57356608478803\n"
     ]
    }
   ],
   "source": [
    "clf= SGDClassifier()\n",
    "benchmark_model('SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "score: 0.5486284289276808\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC(class_weight='balanced')\n",
    "benchmark_model('LinearSVC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 keywords per class:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('negative',\n",
       "  'seriously differently discussion delete disruptive according fraud article arguing does'),\n",
       " ('neutral', 'objected jfg 8213 page doing 2016 link changes versions rfc'),\n",
       " ('positive', 'jr class amusing thank friends fun tie seven concerned good')]"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "if feature_names:\n",
    "    feature_names = np.asarray(feature_names)\n",
    "\n",
    "keywords=[]\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "print(\"top 10 keywords per class:\")\n",
    "for i, label in enumerate(labels):\n",
    "    top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "    #print(\"%s: %s\" % (label, \" \".join(feature_names[top10])))\n",
    "    keywords.append((label, \" \".join(feature_names[top10])))\n",
    "    \n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>top 10 words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>seriously differently discussion delete disrup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>objected jfg 8213 page doing 2016 link changes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>jr class amusing thank friends fun tie seven c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                       top 10 words\n",
       "0  negative  seriously differently discussion delete disrup...\n",
       "1   neutral  objected jfg 8213 page doing 2016 link changes...\n",
       "2  positive  jr class amusing thank friends fun tie seven c..."
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_keywords = pd.DataFrame(keywords, columns=['label', 'top 10 words'])\n",
    "ds_keywords.to_csv(f'../results/keywords_{target}_{lang}.csv', index=False)\n",
    "ds_keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1 micro</th>\n",
       "      <th>f1 macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random</td>\n",
       "      <td>0.473815</td>\n",
       "      <td>0.473815</td>\n",
       "      <td>0.334480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.591022</td>\n",
       "      <td>0.591022</td>\n",
       "      <td>0.274062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.573566</td>\n",
       "      <td>0.573566</td>\n",
       "      <td>0.459578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.548628</td>\n",
       "      <td>0.548628</td>\n",
       "      <td>0.454663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  accuracy  f1 micro  f1 macro\n",
       "0     random  0.473815  0.473815  0.334480\n",
       "1         NB  0.591022  0.591022  0.274062\n",
       "2        SGD  0.573566  0.573566  0.459578\n",
       "3  LinearSVC  0.548628  0.548628  0.454663"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_ds = pd.DataFrame(list(acc_scores.values()), columns=['model', 'accuracy', 'f1 micro', 'f1 macro'])\n",
    "acc_ds.to_csv(f'../results/opinions_f1{target}_{lang}.csv', index=False)\n",
    "acc_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "types = X_test.groupby(['type'])\n",
    "\n",
    "\n",
    "for name, group in types:\n",
    "    for clf_name in list(acc_scores.keys()):\n",
    "        #TODO: use only pos and neg inside groups\n",
    "        fscore=metrics.f1_score(group['y_test'], group[clf_name], average='micro') \n",
    "        X_test.loc[X_test.type==name, 'f1' + clf_name] = fscore\n",
    "        #print(fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>f1random</th>\n",
       "      <th>f1NB</th>\n",
       "      <th>f1SGD</th>\n",
       "      <th>f1LinearSVC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>agreement</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>criticism</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disagreement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doubing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           type  f1random  f1NB  f1SGD  f1LinearSVC\n",
       "0     agreement       0.5   0.0    0.0          0.0\n",
       "1       content       0.0   0.5    0.5          0.5\n",
       "2     criticism       0.5   0.0    1.0          1.0\n",
       "3  disagreement       0.0   0.0    0.0          0.0\n",
       "4       doubing       0.0   0.0    0.0          0.0"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_types = X_test.groupby('type').mean()\n",
    "f1_types = f1_types.reset_index()\n",
    "f1_types = f1_types[['type','f1random', 'f1NB','f1SGD','f1LinearSVC']]\n",
    "f1_types.to_csv(f'../results/opinions_f1{target}_types_{lang}.csv', index=False)\n",
    "f1_types.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading output file\n",
      "predicting target:sentiment, lang:en size:(12104, 33).. done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "out_file = '../dataset/wiki/opinions_predicted.csv'\n",
    "ds_out = None\n",
    "\n",
    "if os.path.exists(out_file):\n",
    "    print('reading output file')\n",
    "    ds_out = pd.read_csv(out_file)\n",
    "else:\n",
    "    print('reading annotated file')\n",
    "    ds_out = pd.read_csv('../dataset/wiki/opinions_annotated.csv')\n",
    "    \n",
    "\n",
    "ix = ds_out.lang==lang\n",
    "X_ds = vectorizer.transform(ds_out[ix].text)\n",
    "ds_out.loc[ix, target + '_pred'] = clf.predict(X_ds)\n",
    "ds_out.to_csv(out_file, index=False)\n",
    "print(f'predicting target:{target}, lang:{lang} size:{ds_out.shape}.. done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'page_id', 'page_title', 'revision_id', 'turn_id',\n",
       "       'contributor', 'timestamp', 'topic', 'raw', 'text', 'type', 'lang',\n",
       "       'creation_dt', 'revision_uid', 'turn_uid', 'page_url', 'country', 'cc2',\n",
       "       'cc3', 'area', 'sent_score', 'sent_magnitude', 'sentiment', 'type1',\n",
       "       'stance1', 'sentiment1', 'type2', 'stance2', 'sentiment2', 'stance',\n",
       "       'sentiment_y', 'stance_pred', 'sentiment_pred'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_out.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 33)\n"
     ]
    }
   ],
   "source": [
    "print(ds_out[ds_out[target + '_pred'].isnull()].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
