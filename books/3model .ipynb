{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sample(ds, field, num=50):\n",
    "    ds_train = ds[ds[field]==1].sample(num)\n",
    "    ds_train = ds_train.append(ds[ds[field]==0].sample(num))\n",
    "    ds_train = ds_train.append(ds[ds[field]==-1].sample(num))\n",
    "    ds_train.shape\n",
    "    return ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(X, y):\n",
    "    model = Pipeline([('vect', CountVectorizer())\n",
    "                        ,('tfidf', TfidfTransformer())\n",
    "                        ,('clf', MultinomialNB()),\n",
    "    ])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "    model = model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def benchmark(clf,X_train, X_test, y_train, y_test):\n",
    "    #print('_' * 80)\n",
    "    #print(\"Training: \")\n",
    "    #print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    #print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    #print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    #print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        #print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        #print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        if False and feature_names is not None:\n",
    "            print(\"top 10 keywords per class:\")\n",
    "            for i, label in enumerate(target_names):\n",
    "                top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "                print(trim(\"%s: %s\" % (label, \" \".join(feature_names[top10]))))\n",
    "        #print()\n",
    "\n",
    "    if False:\n",
    "        #print(\"classification report:\")\n",
    "        print(metrics.classification_report(y_test, pred,\n",
    "                                            target_names=target_names))\n",
    "\n",
    "    if False:\n",
    "        #print(\"confusion matrix:\")\n",
    "        print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    #print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf, clf_descr, score, train_time, test_time, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def benchmark_models(X_train, X_test, y_train, y_test, vectorizer, path):  \n",
    "    \n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for penalty in [\"l2\", \"l1\"]:\n",
    "        print('=' * 80)\n",
    "        print(\"%s penalty\" % penalty.upper())\n",
    "        # Train Liblinear model\n",
    "        results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,\n",
    "                                                dual=False, tol=1e-3),\n",
    "                                X_train, X_test, y_train, y_test))\n",
    "\n",
    "        # Train SGD model\n",
    "        results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                               penalty=penalty), \n",
    "                                X_train, X_test, y_train, y_test))\n",
    "\n",
    "    # Train SGD with Elastic Net penalty\n",
    "    #print('=' * 80)\n",
    "    #print(\"Elastic-Net penalty\")\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                           penalty=\"elasticnet\"),\n",
    "                            X_train, X_test, y_train, y_test))\n",
    "\n",
    "    # Train sparse Naive Bayes classifiers\n",
    "    #print('=' * 80)\n",
    "    #print(\"Naive Bayes\")\n",
    "    results.append(benchmark(MultinomialNB(alpha=.01),\n",
    "                            X_train, X_test, y_train, y_test))\n",
    "    results.append(benchmark(MultinomialNB(),\n",
    "                            X_train, X_test, y_train, y_test))\n",
    "    results.append(benchmark(BernoulliNB(alpha=.01),\n",
    "                            X_train, X_test, y_train, y_test))\n",
    "\n",
    "\n",
    "    #plot_scores(results,path)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_scores(results,path):\n",
    "    \n",
    "    # make some plots\n",
    "    indices = np.arange(len(results))\n",
    "    results = [[x[i] for x in results] for i in range(6)]\n",
    "\n",
    "    clfs, clf_names, score, training_time, test_time, preds = results\n",
    "    training_time = np.array(training_time) / np.max(training_time)\n",
    "    test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"Score\")\n",
    "    plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
    "    plt.barh(indices + .3, training_time, .2, label=\"training time\",\n",
    "             color='c')\n",
    "    plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
    "    plt.yticks(())\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplots_adjust(left=.25)\n",
    "    plt.subplots_adjust(top=.95)\n",
    "    plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "    for i, c in zip(indices, clf_names):\n",
    "        plt.text(-.3, i, c)\n",
    "        \n",
    "    plt.savefig(path, format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_sample(model):\n",
    "    print('evaluating sample data')\n",
    "    docs_new = ['i agree with you', 'i disagree with you']\n",
    "    predicted = model.predict(docs_new)\n",
    "\n",
    "    for doc, stance in zip(docs_new, predicted):\n",
    "        print('%r => %s' % (doc, stance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_stats(clf_names, preds):\n",
    "    print('calculating f-scores')\n",
    "    ds_train['stance_pred'] = model.predict(ds_train.text)\n",
    "    types = ds_train.groupby(['type'])\n",
    "\n",
    "    for name, group in types:\n",
    "        #TODO: use only pos and neg inside groups\n",
    "        fscore=metrics.f1_score(group.stance, group.stance_pred, average='micro') \n",
    "        #f1_macro=metrics.f1_score(group.stance, group.stance_pred, labels=[-1,1], average='macro') \n",
    "        #print(name, fscore)\n",
    "        ds_train.loc[ds_train.type==name, 'fscore_nb'] = fscore\n",
    "        #ds_train.loc[ds_train.type==name, 'fscore_macro'] = f1_macro\n",
    "\n",
    "    f1_micro=metrics.f1_score(ds_train.stance, ds_train.stance_pred, average='micro') \n",
    "    f1_macro=metrics.f1_score(ds_train.stance, ds_train.stance_pred, average='macro') \n",
    "\n",
    "    ds_train['fscore_nb_micro'] = f1_micro\n",
    "    ds_train['fscore_nb_macro'] = f1_macro\n",
    "    \n",
    "    fscores = ds_train.groupby('type').agg({'fscore_nb': 'mean'})\n",
    "    fscores = fscores.reset_index()\n",
    "    fscores.rename(columns={'fscore_nb': 'NB'}, inplace=True)\n",
    "    fscores\n",
    "    # fscores.loc[fscores.shape[0]] = ['F micro' , ds_train.fscore_nb_micro[0]]\n",
    "    # fscores.loc[fscores.shape[0]] = ['F macro' , ds_train.fscore_nb_macro[0]]\n",
    "    fscores['type'] = fscores['type'].str.replace('_', ' ')\n",
    "    #fscores.to_csv('../results/fscores.csv', index=False)\n",
    "    #print(fscores)\n",
    "    \n",
    "    fmscores = ds_train[['fscore_nb_micro', 'fscore_nb_macro']].mean()\n",
    "    fmscores = fmscores.reset_index(name='NB')\n",
    "    f2 = ds_train[['fscore_nb_micro', 'fscore_nb_macro']].mean()\n",
    "    fmscores['index'] = fmscores['index'].str.replace('fscore_nb_' ,'F ')\n",
    "    fmscores['alg2'] = f2.values\n",
    "    fmscores.rename(columns={'index':'F score'}, inplace=True)\n",
    "    #fmscores.to_csv('../results/fmscores.csv', index=False)\n",
    "    #print(fmscores)\n",
    "    return fscores, fmscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def benchmark_stats(X_test, y_test, results):\n",
    "    print('calculating f-scores')\n",
    "    ds_train = X_test.copy()\n",
    "    ds_train['y_test'] = y_test\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    results = heapq.nlargest(5, results, key=lambda x: x[2])\n",
    "\n",
    "    for r in results:\n",
    "        clf_name =r[1]\n",
    "        pred = r[5]\n",
    "        ds_train[clf_name] = pred\n",
    "    \n",
    "    types = ds_train.groupby(['type'])\n",
    "    micro_stats = []\n",
    "    macro_stats = []\n",
    "\n",
    "    for r in results:\n",
    "        clf_name = r[1]\n",
    "        for name, group in types:\n",
    "            #TODO: use only pos and neg inside groups\n",
    "            #print(clf_name, group[clf_name].shape)\n",
    "            fscore=metrics.f1_score(group.y_test, group[clf_name], average='micro') \n",
    "            #f1_macro=metrics.f1_score(group.stance, group.stance_pred, labels=[-1,1], average='macro') \n",
    "            #print(name, fscore)\n",
    "            \n",
    "            stat_name='fscore_'+clf_name\n",
    "            if not stat_name in micro_stats:\n",
    "                micro_stats.append(stat_name)\n",
    "            ds_train.loc[ds_train.type==name, stat_name] = fscore\n",
    "            #ds_train.loc[ds_train.type==name, 'fscore_macro'] = f1_macro\n",
    "            #print(len(ds_train.columns))\n",
    "\n",
    "        f1_micro=metrics.f1_score(ds_train.y_test, ds_train[clf_name], average='micro') \n",
    "        f1_macro=metrics.f1_score(ds_train.y_test, ds_train[clf_name], average='macro') \n",
    "        stat_name='fscore_micro'+clf_name\n",
    "        macro_stats.append(stat_name)\n",
    "        ds_train[stat_name] = f1_micro\n",
    "        stat_name='fscore_macro'+clf_name\n",
    "        macro_stats.append(stat_name)\n",
    "        ds_train[stat_name] = f1_macro\n",
    "    \n",
    "    micro_stats.append('type')\n",
    "    fscores = ds_train[micro_stats].groupby('type').mean()\n",
    "    fscores = fscores.reset_index()\n",
    "    #fscores.rename(columns={'fscore_nb': 'NB'}, inplace=True)\n",
    "    fscores['type'] = fscores['type'].str.replace('_', ' ')\n",
    "    fscores['type'] = fscores['type'].str.replace('+', ' and ')\n",
    "    cols = [c.replace('fscore_', '') for c in fscores.columns]\n",
    "    fscores.columns = cols\n",
    "    #print(len(fscores.columns))\n",
    "    \n",
    "    fmscores = ds_train[macro_stats].mean()\n",
    "    fmscores = fmscores.reset_index()\n",
    "    fmscores.columns = ['stat', 'value']\n",
    "    fmscores['stat'] = fmscores['stat'].str.replace('fscore_micro' ,'F micro ')\n",
    "    fmscores['stat'] = fmscores['stat'].str.replace('fscore_macro' ,'F macro ')\n",
    "#     fmscores['alg2'] = f2.values\n",
    "#     fmscores.rename(columns={'index':'F score'}, inplace=True)\n",
    "    #fmscores.to_csv('../results/fmscores.csv', index=False)\n",
    "    #print(fmscores)\n",
    "    return fscores, fmscores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our english dataset...\n",
      "stance classification\n",
      "================================================================================\n",
      "L2 penalty\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='l2', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l2', random_state=None, tol=0.001, verbose=0)\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "================================================================================\n",
      "L1 penalty\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='l2', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l1', random_state=None, tol=0.001, verbose=0)\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "Training: \n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "Training: \n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "calculating f-scores\n",
      "(75,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/svm/classes.py:199: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/svm/classes.py:199: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.001,\n",
      "     verbose=0)\n",
      "sentiment classification\n",
      "================================================================================\n",
      "L2 penalty\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='l2', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l2', random_state=None, tol=0.001, verbose=0)\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "================================================================================\n",
      "L1 penalty\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='l2', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l1', random_state=None, tol=0.001, verbose=0)\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "Training: \n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "Training: \n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "calculating f-scores\n",
      "(12,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/svm/classes.py:199: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/svm/classes.py:199: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "print('our english dataset...')\n",
    "ds = pd.read_csv('../dataset/wiki/opinions_annotated.csv')\n",
    "ds = ds[ds.lang=='en']\n",
    "\n",
    "print('stance classification')\n",
    "ds_train = get_sample(ds, 'stance')\n",
    "X = ds_train[['text', 'type']]\n",
    "y = ds_train.stance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "#vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,stop_words='english')\n",
    "vectorizer = TfidfVectorizer()\n",
    "results = benchmark_models(X_train.text, X_test.text, y_train, y_test, vectorizer, '../results/opinions_stance_score_en.eps')\n",
    "fscores, fmscores = benchmark_stats(X_test, y_test, results)\n",
    "#choose the best model\n",
    "#model = build_model(X,y)\n",
    "model = heapq.nlargest(1, results, key=lambda x: x[2])[0][0]\n",
    "print('best model: ' + str(model))\n",
    "#test_sample(model)\n",
    "X = vectorizer.transform(ds.text.values)\n",
    "#X = ds.text\n",
    "predicted = model.predict(X)\n",
    "ds['stance_pred'] = predicted\n",
    "fscores.to_csv('../results/opinions_fscores_stance_en.csv', index=False)\n",
    "fmscores.to_csv('../results/opinions_fmscores_stance_en.csv', index=False)\n",
    "\n",
    "print('sentiment classification')\n",
    "ds_train = get_sample(ds, 'sentiment', 8)\n",
    "X = ds_train[['text', 'type']]\n",
    "y = ds_train.sentiment\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "#vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,stop_words='english')\n",
    "vectorizer = TfidfVectorizer()\n",
    "results = benchmark_models(X_train.text, X_test.text, y_train, y_test, vectorizer, '../results/opinions_sentiment_score_en.eps')\n",
    "fscores, fmscores = benchmark_stats(X_test, y_test, results)\n",
    "#choose the best model\n",
    "#model = build_model(X,y)\n",
    "model = heapq.nlargest(1, results, key=lambda x: x[2])[0][0]\n",
    "print('best model: ' + str(model))\n",
    "#test_sample(model)\n",
    "X = vectorizer.transform(ds.text.values)\n",
    "#X = ds.text\n",
    "predicted = model.predict(X)\n",
    "ds['sentiment_pred'] = predicted\n",
    "fscores.to_csv('../results/opinions_fscores_sent_en.csv', index=False)\n",
    "fmscores.to_csv('../results/opinions_fmscores_sent_en.csv', index=False)\n",
    "\n",
    "ds.to_csv('../dataset/wiki/opinions_predicted_en.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('our spanish dataset...')\n",
    "ds = pd.read_csv('../dataset/wiki/opinions_annotated.csv')\n",
    "ds = ds[ds.lang=='es']\n",
    "\n",
    "\n",
    "print('stance classification')\n",
    "ds_train = get_sample(ds, 'stance', 50)\n",
    "X = ds_train[['text', 'type']]\n",
    "y = ds_train.stance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "#vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,stop_words='english')\n",
    "vectorizer = TfidfVectorizer()\n",
    "results = benchmark_models(X_train.text, X_test.text, y_train, y_test, vectorizer, '../results/opinions_stance_score_es.eps')\n",
    "fscores, fmscores = benchmark_stats(X_test, y_test, results)\n",
    "#choose the best model\n",
    "#model = build_model(X,y)\n",
    "model = heapq.nlargest(1, results, key=lambda x: x[2])[0][0]\n",
    "print('best model: ' + str(model))\n",
    "#test_sample(model)\n",
    "X = vectorizer.transform(ds.text.values)\n",
    "#X = ds.text\n",
    "predicted = model.predict(X)\n",
    "ds['stance_pred'] = predicted\n",
    "fscores.to_csv('../results/opinions_fscores_stance_es.csv', index=False)\n",
    "fmscores.to_csv('../results/opinions_fmscores_stance_es.csv', index=False)\n",
    "\n",
    "print('sentiment classification')\n",
    "ds_train = get_sample(ds, 'sentiment', 4)\n",
    "X = ds_train[['text', 'type']]\n",
    "y = ds_train.sentiment\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "#vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,stop_words='english')\n",
    "vectorizer = TfidfVectorizer()\n",
    "results = benchmark_models(X_train.text, X_test.text, y_train, y_test, vectorizer, '../results/opinions_sentiment_score_es.eps')\n",
    "fscores, fmscores = benchmark_stats(X_test, y_test, results)\n",
    "#choose the best model\n",
    "#model = build_model(X,y)\n",
    "model = heapq.nlargest(1, results, key=lambda x: x[2])[0][0]\n",
    "print('best model: ' + str(model))\n",
    "#test_sample(model)\n",
    "X = vectorizer.transform(ds.text.values)\n",
    "#X = ds.text\n",
    "predicted = model.predict(X)\n",
    "ds['sentiment_pred'] = predicted\n",
    "fscores.to_csv('../results/opinions_fscores_sent_es.csv', index=False)\n",
    "fmscores.to_csv('../results/opinions_fmscores_sent_es.csv', index=False)\n",
    "\n",
    "ds.to_csv('../dataset/wiki/opinions_predicted_es.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('aawd dataset...')\n",
    "ds = pd.read_csv('../dataset/wiki/aawd_preprocessed.csv')\n",
    "#ds = ds[ds.lang=='en']\n",
    "ds_train = get_sample(ds, 'stance', 300)\n",
    "X = ds_train[['text', 'type']]\n",
    "print('stance classification')\n",
    "y = ds_train.stance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "#vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,stop_words='english')\n",
    "vectorizer = TfidfVectorizer()\n",
    "results = benchmark_models(X_train.text, X_test.text, y_train, y_test, vectorizer, '../results/awwd_stance_score.eps')\n",
    "fscores, fmscores = benchmark_stats(X_test, y_test, results)\n",
    "#choose the best model\n",
    "model = build_model(X=ds_train.text,y = ds_train.stance)\n",
    "#model = heapq.nlargest(1, results, key=lambda x: x[2])\n",
    "test_sample(model)\n",
    "#X = vectorizer.transform(ds.text.values)\n",
    "X = ds.text\n",
    "predicted = model.predict(X)\n",
    "ds['stance_pred'] = predicted\n",
    "\n",
    "ds.to_csv('../dataset/wiki/aawd_predicted.csv', index=False)\n",
    "fscores.to_csv('../results/aawd_fscores_stance.csv', index=False)\n",
    "fmscores.to_csv('../results/aawd_fmscores_stance.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
